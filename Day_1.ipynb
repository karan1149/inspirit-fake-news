{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ATa2miB-8VIW"
   },
   "source": [
    "# Day 1: Introduction to Fake News Classification\n",
    "\n",
    "In this project, we will learn how to build a classifier that is able to distinguish fake news websites from established news websites. The fake news problem is important: many argue that the unchecked spread of misinformation played a significant factor in the outcome of the 2016 U.S. election, along with other elections elsewhere. This is also a hard problem, one that we will certainly not completely solve in the next few daysâ€“the task of separating truth from fiction is a difficult research task today. \n",
    "\n",
    "That said, we will find that fake news websites often have obvious \"tells\" that suggest they may not be as respectable as established news websites. Without attempting to discern whether a specific news story on a site is true or false, we may be able to get a clue by studying these \"tells\". We will then use this insight to classify new fake news websites!\n",
    "\n",
    "Today, we will begin by better understanding how websites work under the hood so that we are better equipped to identify signs that a website might not be reliable. Next, we introduce a dataset of news websites labeled real or fake. Finally, we investigate different \"tells\" that may suggest a website is unreliable.\n",
    "\n",
    "Run the below cell to get started!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6qPRfrpe8NcU"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pickle\n",
    "  \n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# Download class resources...\n",
    "r = requests.get(\"https://www.dropbox.com/s/2pj07qip0ei09xt/inspirit_fake_news_resources.zip?dl=1\")\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall()\n",
    "\n",
    "basepath = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fj3IADlNGjUg"
   },
   "source": [
    "## Anatomy of a (Fake) News Website\n",
    "\n",
    "Have you ever wondered how websites like *google.com* and *nytimes.com* work under the hood? Using the internet every day, it is easy to forget how magical even the most mundane web browsing experiences are. Consider, for example, this article on the New York Times:\n",
    "\n",
    "![NYTimes Article](https://karansinghal.com/public/inspirit/nytimes-article.png)\n",
    "\n",
    "\n",
    "How does the browser know to show the title of the article near the top of the page? How does it know that the word \"Opinion\" should be left-centered and gray-colored? How does it know where to find the image to display?\n",
    "\n",
    "All of these questions can be answered by probing through the HTML of a webpage. HTML is a simple markup language that augments text with the structure you'd expect from a webpage. It's the language that provides the structure for every webpage you see. Here's an example of an HTML document for a simple webpage.\n",
    "\n",
    "![HTML Example](https://karansinghal.com/public/inspirit/html-example.png)\n",
    "\n",
    "You can [play around with this specific example in an interactive environment](https://www.w3schools.com/html/tryit.asp?filename=tryhtml_default). Also read [this short, basic introduction to HTML](https://www.w3schools.com/html/html_intro.asp).\n",
    "\n",
    "Every webpage you see on the web has an associated source HTML document, and you can [view this yourself if you like](https://www.computerhope.com/issues/ch000746.htm). For example, the earlier New York Times article, has an HTML document that instructs the browser to format the text according to what you see on the webpage.\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Given the URL of a news website and its HTML, can we classify the news website as either fake or real? Note that this is not a fine-grained task: we are not classifying individual articles, but rather the homepage of the corresponding news websites. We are not even attempting to determine the truth value of individual stories, and this is a key limitation of our approach. However, we will find that we will be able to achieve surprisingly solid results on this task using relatively simple models, given some clever feature selection.\n",
    "\n",
    "## Dataset \n",
    "\n",
    "As we've seen, machine learning models are typically trained using groundtruth data split into train, dev, and test data. Where can we get groundtruth real and fake news websites? We use an independent third-party, OpenSources. Given a list of websites and their labels, we scrape them to get their HTML, which we will use to identify features that are useful for classification (this has already been done by the teaching team behind the scenes). We scrape each website several times over the course of days, ensuring that we are not overfitting to the news stories of a specific day. Thus, each website provides multiple labeled data points.\n",
    "\n",
    "Finally, we split the data into train, val, and test by assigning 80% of the websites to train, 10% to val (a.k.a. dev), and 10% to test. The test data is hidden from you for now. Each portion of the data includes about 50% real and fake news websites, so there isn't a large data imbalance. We ensure that different examples (on different timesteps) for the same website are in the same portion of the data. Why is this important to ensure that our val and test accuracy are representative of predictions on unseen websites?\n",
    "\n",
    "Load the train and val in the below cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 82
    },
    "colab_type": "code",
    "id": "0En5nk5pEc72",
    "outputId": "a2c273e2-8253-488e-c532-d0fd837d00a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train examples: 772\n",
      "Number of val examples: 90\n",
      "Fraction of train examples that are fake: 0.533678756476684\n",
      "Fraction of val examples that are fake: 0.5555555555555556\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(basepath, 'sample_train_val_data.pkl'), 'rb') as f:\n",
    "  train_data, val_data = pickle.load(f)\n",
    "\n",
    "print('Number of train examples:', len(train_data))\n",
    "print('Number of val examples:', len(val_data))\n",
    "\n",
    "print('Fraction of train examples that are fake:', len([datapoint for datapoint in train_data if datapoint[2] == 0]) / float(len(train_data)))\n",
    "print('Fraction of val examples that are fake:', len([datapoint for datapoint in val_data if datapoint[2] == 0]) / float(len(val_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L4yi_xDAEdTN"
   },
   "source": [
    "We can see that the number of examples for each portion of the data approximately matches the split above, and each portion has roughly 50% fake news websites. Now to explore what each data point looks like. Spend ~5 minutes browsing through the data by changing example_idx below. You are able to see the URL, label (0 is real, 1 is fake), and part of the HTML for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "colab_type": "code",
    "id": "JEi_NJeuIeBf",
    "outputId": "d2b04ba2-1663-48a6-ca4d-5714b7ae3c41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of values per data point: 3\n",
      "\n",
      "URL for chosen example: www.motherjones.com\n",
      "Label for chosen example: 0\n",
      "HTML for chosen example (first 5000 chars):\n",
      "\n",
      " <!DOCTYPE html>\n",
      "<html class=\"no-js\" lang=\"en-US\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <script type=\"text/javascript\">\n",
      "   window.NREUM||(NREUM={}),__nr_require=function(e,n,t){function r(t){if(!n[t]){var o=n[t]={exports:{}};e[t][0].call(o.exports,function(n){var o=e[t][1][n];return r(o||n)},o,o.exports)}return n[t].exports}if(\"function\"==typeof __nr_require)return __nr_require;for(var o=0;o<t.length;o++)r(t[o]);return r}({1:[function(e,n,t){function r(){}function o(e,n,t){return function(){return i(e,[c.now()].concat(u(arguments)),n?null:this,t),n?void 0:this}}var i=e(\"handle\"),a=e(3),u=e(4),f=e(\"ee\").get(\"tracer\"),c=e(\"loader\"),s=NREUM;\"undefined\"==typeof window.newrelic&&(newrelic=s);var p=[\"setPageViewName\",\"setCustomAttribute\",\"setErrorHandler\",\"finished\",\"addToTrace\",\"inlineHit\",\"addRelease\"],d=\"api-\",l=d+\"ixn-\";a(p,function(e,n){s[n]=o(d+n,!0,\"api\")}),s.addPageAction=o(d+\"addPageAction\",!0),s.setCurrentRouteName=o(d+\"routeName\",!0),n.exports=newrelic,s.interaction=function(){return(new r).get()};var m=r.prototype={createTracer:function(e,n){var t={},r=this,o=\"function\"==typeof n;return i(l+\"tracer\",[c.now(),e,t],r),function(){if(f.emit((o?\"\":\"no-\")+\"fn-start\",[c.now(),r,o],t),o)try{return n.apply(this,arguments)}catch(e){throw f.emit(\"fn-err\",[arguments,this,e],t),e}finally{f.emit(\"fn-end\",[c.now()],t)}}}};a(\"actionText,setName,setAttribute,save,ignore,onEnd,getContext,end,get\".split(\",\"),function(e,n){m[n]=o(l+n)}),newrelic.noticeError=function(e,n){\"string\"==typeof e&&(e=new Error(e)),i(\"err\",[e,c.now(),!1,n])}},{}],2:[function(e,n,t){function r(e,n){if(!o)return!1;if(e!==o)return!1;if(!n)return!0;if(!i)return!1;for(var t=i.split(\".\"),r=n.split(\".\"),a=0;a<r.length;a++)if(r[a]!==t[a])return!1;return!0}var o=null,i=null,a=/Version\\/(\\S+)\\s+Safari/;if(navigator.userAgent){var u=navigator.userAgent,f=u.match(a);f&&u.indexOf(\"Chrome\")===-1&&u.indexOf(\"Chromium\")===-1&&(o=\"Safari\",i=f[1])}n.exports={agent:o,version:i,match:r}},{}],3:[function(e,n,t){function r(e,n){var t=[],r=\"\",i=0;for(r in e)o.call(e,r)&&(t[i]=n(r,e[r]),i+=1);return t}var o=Object.prototype.hasOwnProperty;n.exports=r},{}],4:[function(e,n,t){function r(e,n,t){n||(n=0),\"undefined\"==typeof t&&(t=e?e.length:0);for(var r=-1,o=t-n||0,i=Array(o<0?0:o);++r<o;)i[r]=e[n+r];return i}n.exports=r},{}],5:[function(e,n,t){n.exports={exists:\"undefined\"!=typeof window.performance&&window.performance.timing&&\"undefined\"!=typeof window.performance.timing.navigationStart}},{}],ee:[function(e,n,t){function r(){}function o(e){function n(e){return e&&e instanceof r?e:e?f(e,u,i):i()}function t(t,r,o,i){if(!d.aborted||i){e&&e(t,r,o);for(var a=n(o),u=v(t),f=u.length,c=0;c<f;c++)u[c].apply(a,r);var p=s[y[t]];return p&&p.push([b,t,r,a]),a}}function l(e,n){h[e]=v(e).concat(n)}function m(e,n){var t=h[e];if(t)for(var r=0;r<t.length;r++)t[r]===n&&t.splice(r,1)}function v(e){return h[e]||[]}function g(e){return p[e]=p[e]||o(t)}function w(e,n){c(e,function(e,t){n=n||\"feature\",y[t]=n,n in s||(s[n]=[])})}var h={},y={},b={on:l,addEventListener:l,removeEventListener:m,emit:t,get:g,listeners:v,context:n,buffer:w,abort:a,aborted:!1};return b}function i(){return new r}function a(){(s.api||s.feature)&&(d.aborted=!0,s=d.backlog={})}var u=\"nr@context\",f=e(\"gos\"),c=e(3),s={},p={},d=n.exports=o();d.backlog=s},{}],gos:[function(e,n,t){function r(e,n,t){if(o.call(e,n))return e[n];var r=t();if(Object.defineProperty&&Object.keys)try{return Object.defineProperty(e,n,{value:r,writable:!0,enumerable:!1}),r}catch(i){}return e[n]=r,r}var o=Object.prototype.hasOwnProperty;n.exports=r},{}],handle:[function(e,n,t){function r(e,n,t,r){o.buffer([e],r),o.emit(e,n,t)}var o=e(\"ee\").get(\"handle\");n.exports=r,r.ee=o},{}],id:[function(e,n,t){function r(e){var n=typeof e;return!e||\"object\"!==n&&\"function\"!==n?-1:e===window?0:a(e,i,function(){return o++})}var o=1,i=\"nr@id\",a=e(\"gos\");n.exports=r},{}],loader:[function(e,n,t){function r(){if(!E++){var e=x.info=NREUM.info,n=l.getElementsByTagName(\"script\")[0];if(setTimeout(s.abort,3e4),!(e&&e.licenseKey&&e.applicationID&&n))return s.abort();c(y,function(n,t){e[n]||(e[n]=t)}),f(\"mark\",[\"onload\",a()+x.offset],null,\"api\");var t=l.createElement(\"script\");t.src=\"https://\"+e.agent,n.parentNode.insertBefore(t,n)}}function o(){\"complete\"===l.readyState&&i()}function i(){f(\"mark\",[\"domContent\",a()+x.offset],null,\"api\")}function a(){return O.exists&&performance.now?Math.round(performance.now()):(u=Math.max((new Date).getTime(),u))-x.offset}var u=(new Date).getTime(),f=e(\"handle\"),c=e(3),s=e(\"ee\"),p=e(2),d=window,l=d.document,m=\"addEventListener\",v=\"attachEvent\",g=d.XMLHttpRequest,w=g&&g.prototype;NREUM.o={ST:setTimeout,SI:d.setImmediate,CT:clearTimeout,XHR:g,REQ:d.Request,EV:d.Event,PR:d.Promise,MO:d.MutationObserver};var h=\"\"+location,y={beacon:\"bam.nr-data.net\",errorBeacon:\"bam.nr-data.net\",agent:\"js-agent.newrelic.com/nr-1123.min.js\"},b=g&&w&&w[m]&&!/CriOS/.test(navigator.userAgent),x=n.exports={offset:u,now:a,origin:h,features:{},xhrWrappable:b,userAgent:p};e(1),\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "example_idx = 0\n",
    "### END CODE HERE ###\n",
    "\n",
    "print('Number of values per data point: %d\\n' % len(train_data[0]))\n",
    "\n",
    "print('URL for chosen example:', train_data[example_idx][0])\n",
    "print('Label for chosen example:', train_data[example_idx][2])\n",
    "print('HTML for chosen example (first 5000 chars):\\n\\n', bs(train_data[example_idx][1]).prettify()[:5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FiDpu0qyBiW8"
   },
   "source": [
    "Observe that each data point has three values: the URL, the HTML, and the binary (0 or 1) label. A label of \"1\" indicates that the website is a fake news website, and a label of \"0\" indicates that the website does not have fake news. See if you can spot some differences between examples with label 0 and examples with label 1, especially in their URLs! The HTML may be a bit difficult to read, since it is much longer, so don't worry about this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6tEIBfLXFxNn"
   },
   "source": [
    "## Probing Hypotheses\n",
    "\n",
    "Browsing through the examples above, you might have gotten a few ideas for differences between real and fake news websites. For instance, you might have noticed that many fake news websites use domain name extensions other than \".com\", whereas this is less common for real news websites.\n",
    "\n",
    "How do we test this hypothesis in a rigorous way? One way would be to calculate what fraction of fake websites have \".com\" domain extensions, along with the fraction of real websites that have \".com\" domain extensions, and compare these numbers (by taking their ratio of fake fraction to real fraction). \n",
    "\n",
    "If the ratio is less than 1, then we have reason to believe that real news websites disporportionately use \".com\" extensions, and knowing this would be useful for classification. If the ratio is greater than 1, then we know the same of fake news websites, and this is still useful for separating out real and fake news websites. If the ratio is 1, this means that our hypothesis isn't very useful for separating out real and fake news websites, at least not by itself.\n",
    "\n",
    "For those of you with some probability background, this ratio is important in updating probabilities using Bayes Theorem, and basically corresponds to how informative something is in telling us whether a website if fake or not. If you're unfamiliar with this, don't worry, it's not important!\n",
    "\n",
    "We define a function below that returns the real and fake fractions of the training data that satisfy a hypothesis. In our code, our hypotheses will just be simple functions that take in a single data point and return \"True\" or \"False\". Make sure you understand what the code is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sXlQXdtwKHJD"
   },
   "outputs": [],
   "source": [
    "def get_real_and_fake_fractions(train_data, hypothesis):\n",
    "    real_true = 0.0\n",
    "    real_total = 0.0\n",
    "    fake_true = 0.0\n",
    "    fake_total = 0.0\n",
    "    \n",
    "    for datapoint in train_data:\n",
    "        # Each datapoint has URL, HTML, label in that order.\n",
    "        label = datapoint[2]\n",
    "        hypothesis_truth = int(hypothesis(datapoint))\n",
    "        if label: # Fake\n",
    "            fake_total += 1\n",
    "            fake_true += hypothesis_truth \n",
    "        else: # Real\n",
    "            real_total += 1\n",
    "            real_true += hypothesis_truth\n",
    "            \n",
    "    return real_true / real_total, fake_true / fake_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NrFNz_XAM_0r"
   },
   "source": [
    "Now, play around with this demonstration that asks you for a domain name extension, and prints out the real fraction, the fake fraction, and the ratio of fake fraction to real fraction. Make sure you understand what the code is doing! After running initially, try other values, like \".org\", \".co.uk\", and \".edu\"! The printed values will update automatically. Note that in some cases, the ratio may be \"Infinity\", if no real websites in the training data have that domain name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66
    },
    "colab_type": "code",
    "id": "Mfim5xsSNYAP",
    "outputId": "dedd97ab-f6e4-4912-b078-9b98821c156b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real fraction: 0.9150485436893204\n",
      "Fake fraction: 0.7944444444444444\n",
      "Ratio fraction: 0.868199233716475\n"
     ]
    }
   ],
   "source": [
    "#@title Run this cell with your hypothesis domain name extension { run: \"auto\" }\n",
    "\n",
    "def domain_extension_hypothesis(datapoint):\n",
    "  extension = \".com\" #@param {type:\"string\"}\n",
    "  url = datapoint[0]\n",
    "  return url.endswith(extension)\n",
    "  \n",
    "real_fraction, fake_fraction = get_real_and_fake_fractions(train_data, \n",
    "                                                           domain_extension_hypothesis)\n",
    "\n",
    "print('Real fraction:', real_fraction)\n",
    "print('Fake fraction:', fake_fraction)\n",
    "\n",
    "def prettify_ratio(ratio):\n",
    "    ratio = (fake_fraction / real_fraction) if real_fraction > 0 else 'Infinity'\n",
    "    if fake_fraction == real_fraction:\n",
    "      ratio = 1\n",
    "    return ratio\n",
    "  \n",
    "print('Ratio fraction:', prettify_ratio(ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you find a domain name extension that produces ratio fraction Infinity? Can you find one that produces ratio fraction 0 (~3 minutes)? Fill them in below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "domain_name_extension_with_ratio_infinity = ''\n",
    "domain_name_extension_with_ratio_zero = ''\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try building a more powerful hypothesis that tests things besides domain name extension. Remember, a hypothesis has to be true or false for a specific website, so we are able to calculate the fraction of fake and real websites that satisfy a hypothesis.\n",
    "\n",
    "One natural idea is counting whether the frequency of words in the HTML of a webpage is above a certain threshold. For example, given the word \"Clinton\" and a threshold of 3, does nytimes.com mention \"Clinton\" 3 times? Does infowars.com? This may tell us something about how useful the word \"Clinton\" is for telling us whether a website is fake or not.\n",
    "\n",
    "Now, code up the below hypothesis function that tests whether the count of a provided word is above a threshold and play with the resulting demo (~15 minutes). We have provided some starter code for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real fraction: 0.4320388349514563\n",
      "Fake fraction: 0.09722222222222222\n",
      "Ratio fraction: 0.2250312109862672\n"
     ]
    }
   ],
   "source": [
    "#@title Run this cell with a word and a threshold { run: \"auto\" }\n",
    "\n",
    "def get_count_from_html(html, hypothesis_word):\n",
    "    # Transform word to lowercase for consistent results.\n",
    "    return html.count(hypothesis_word.lower())\n",
    "\n",
    "def word_threshold_hypothesis(datapoint):\n",
    "  hypothesis_word = \"opinion\" #@param {type:\"string\"}\n",
    "  threshold = 3 #@param {type:\"integer\"}\n",
    "  # Transform HTML to lowercase for consistent results.\n",
    "  html = datapoint[1].lower() \n",
    "    \n",
    "  ### YOUR CODE HERE ### (Use get_count_from_html!)\n",
    "  count = get_count_from_html(html, hypothesis_word)\n",
    "  return count > threshold\n",
    "  ### END CODE HERE ###\n",
    "  \n",
    "real_fraction, fake_fraction = get_real_and_fake_fractions(train_data, \n",
    "                                                           word_threshold_hypothesis)\n",
    "\n",
    "print('Real fraction:', real_fraction)\n",
    "print('Fake fraction:', fake_fraction)\n",
    "  \n",
    "print('Ratio fraction:', prettify_ratio(ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ypCqOW89OsY-"
   },
   "source": [
    "Once you have \"Clinton\" working with a threshold of 3, try other words, like \"Trump\", \"Obama\", \"Sports\", \"Finance\", and \"Opinion\". Why do you see what you see?\n",
    "\n",
    "Share your most interesting hypothesis word and threshold combinations with the class for discussion!\n",
    "\n",
    "Now, create your own custom hypotheses! All you should change is the hypothesis function (~15 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y02Bmix5O8Ph"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OJsHZUqBPBOY"
   },
   "source": [
    "Once you are done, share your most interesting hypotheses with the class to discuss!\n",
    "\n",
    "Congratulations on completing this notebook! Tomorrow, we'll use the insights you just built up to build our baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Day 1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
