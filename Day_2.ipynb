{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gVgIxW-YPeQH"
   },
   "source": [
    "# Day 2: Creating a Baseline Model for Fake News Classification\n",
    "\n",
    "Yesterday, we investigated several hypotheses for \"tells\" that could be used to separate out real and fake news websites in our dataset without actually determining the truth value of individual articles. This was a big first step towards our goal of doing coarse-grained fake news classification.\n",
    "\n",
    "Today, we build off of the insights we gleaned to build a baseline model using logistic regression. \n",
    "\n",
    "Why build a baseline? Building a baseline provides a benchmark for further work on a task–if you can do well with a simple model, this tells us that even with more sophisticated model architectures, we might have diminishing returns. On the other hand, if our baseline does poorly, this may provide indication that our task/dataset is malformed, or that more complex architectures are necessary. \n",
    "\n",
    "In our case, we will find that building a baseline will show us that the problem of identifying fake news websites is approachable without modeling the truth content of individual articles, a useful insight. Importantly, we will see that using logistic regression in particular gives us a strong foundation for interpreting and improving our model. \n",
    "\n",
    "Run the below cell to get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Unyn9Nek4qtt"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "import requests, io, zipfile\n",
    "# Download class resources...\n",
    "r = requests.get(\"https://www.dropbox.com/s/2pj07qip0ei09xt/inspirit_fake_news_resources.zip?dl=1\")\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall()\n",
    "\n",
    "basepath = '.'\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8x4L6VvhR6wp"
   },
   "source": [
    "## Why Logistic Regression?\n",
    "\n",
    "We've just spent the last week or so learning about more sophisticated neural network architectures. Why should we begin working on a complicated task like fake news classification using such a simple model? Remember that logistic regression is just linear regression followed by a sigmoid function. See [here](https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc) for a detailed review of logistic regression.\n",
    "\n",
    "First, as suggested above, using a simpler model tells us how much room we have to improve. \n",
    "\n",
    "Second, a simple model makes iteration quick and easy–we'll see that for the project of classifying a website based on its URL and HTML, cleverly extracting features from the URL and HTML will be important for our success. Using a model that trains and evaluates quickly is essential for rapid feature selection. \n",
    "\n",
    "Lastly, and perhaps most importantly, logistic regression is *interpretable*. You may have heard in the past that one thing deep neural networks struggle with is interpretability–when you are using these models to make predictions that affect people's wellbeing (e.g., sentencing decisions, predictive policing decisions), it becomes extremely important that you are able to understand why a model is making the predictions it makes. Making deeper neural networks more interpretable is an active area of AI research. This is important for fake news classification as well–as we know in the case of Facebook, poorly filtering out misinformation on social media might even affect elections. \n",
    "\n",
    "However, for simpler models like logistic regression, we get interpretability for free! More on this below, but for now know that when engineering features for logistic regression, we will be able to examine which features correspond most with fake news websites, and which features correspond most with real news websites.\n",
    "\n",
    "## Building Our First Baseline\n",
    "\n",
    "Then our task is to take URL, HTML pairs, turn them into a series of numerical features, and then input them into a logistic regression classifier along with training labels. The tricky part here is finding features that are informative for predicting whether a website is fake or not. Luckily, this is what we worked on yesterday!\n",
    "\n",
    "Yesterday, we found that we could extract some features related to the domain name extension of a website, and they were often informative about whether a website is fake or not. For example, you may have noticed that both fake and real news websites use the \".org\" domain name extension, but fake news websites use it more frequently (perhaps contrary to what you'd expect).\n",
    "\n",
    "Below we introduce some code for taking our training and val data and producing X, y examples that can be fit by a logistic regression model. This code extracts a few basic features from the domain name extension of the website. Your task: add features testing whether the domain name ends in \".co\", \".tv\", and \".news\", according to the template below (~5 minutes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6aFOl-xHCTy7"
   },
   "outputs": [],
   "source": [
    "def prepare_data(data, featurizer):\n",
    "    X = []\n",
    "    y = []\n",
    "    for datapoint in data:\n",
    "        url, html, label = datapoint\n",
    "        # We convert all text in HTML to lowercase, so <p>Hello.</p> is mapped to\n",
    "        # <p>hello</p>. This will help us later when we extract features from \n",
    "        # the HTML, as we will be able to rely on the HTML being lowercase.\n",
    "        html = html.lower() \n",
    "        y.append(label)\n",
    "\n",
    "        features = featurizer(url, html)\n",
    "\n",
    "        # Gets the keys of the dictionary as descriptions, gets the values\n",
    "        # as the numerical features. Don't worry about exactly what zip does!\n",
    "        feature_descriptions, feature_values = zip(*features.items())\n",
    "\n",
    "        X.append(feature_values)\n",
    "\n",
    "    return X, y, feature_descriptions\n",
    "  \n",
    "# Returns a dictionary mapping from plaintext feature descriptions to numerical\n",
    "# features for a (url, html) pair.\n",
    "def domain_featurizer(url, html):\n",
    "    features = {}\n",
    "    \n",
    "    # Binary features for the domain name extension.\n",
    "    features['.com domain'] = url.endswith('.com')\n",
    "    features['.org domain'] = url.endswith('.org')\n",
    "    features['.net domain'] = url.endswith('.net')\n",
    "    features['.info domain'] = url.endswith('.info')\n",
    "    features['.org domain'] = url.endswith('.org')\n",
    "    features['.biz domain'] = url.endswith('.biz')\n",
    "    features['.ru domain'] = url.endswith('.ru')\n",
    "    features['.co.uk domain'] = url.endswith('.co.uk')\n",
    "    \n",
    "    ### YOUR CODE HERE ###\n",
    "    \n",
    "    features['.co domain'] = url.endswith('.co')\n",
    "    features['.tv domain'] = url.endswith('.tv')\n",
    "    features['.news domain'] = url.endswith('.news')\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RncfRR2uzNxg"
   },
   "source": [
    "Make sure you understand what the code above is doing. It produces X, y such that X contains a list of features for each site in the dataset, and y contains the labels in corresponding order. *feature_descriptions* is a list of the names of features (.e.g., '.com domain'). This will be important later when we want to know the names of features when interpreting the model. Let's run our code for processing the data on the train and val sets from yesterday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "colab_type": "code",
    "id": "oVf2aAP7z1Ze",
    "outputId": "110b1d3b-63af-477d-a35f-94b8c8690614"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train examples: 772\n",
      "Number of val examples: 90\n",
      "Number of features per example: 10\n",
      "Feature descriptions:\n",
      "('.com domain', '.org domain', '.net domain', '.info domain', '.biz domain', '.ru domain', '.co.uk domain', '.co domain', '.tv domain', '.news domain')\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(basepath, 'sample_train_val_data.pkl'), 'rb') as f: # TODO change this to actual data\n",
    "  train_data, val_data = pickle.load(f)\n",
    "  \n",
    "print('Number of train examples:', len(train_data))\n",
    "print('Number of val examples:', len(val_data))\n",
    "  \n",
    "train_X, train_y, feature_descriptions = prepare_data(train_data, domain_featurizer)\n",
    "val_X, val_y, feature_descriptions = prepare_data(val_data, domain_featurizer)\n",
    "\n",
    "print('Number of features per example:', len(train_X[0]))\n",
    "print('Feature descriptions:')\n",
    "print(feature_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MA0H-ezT3A3-"
   },
   "source": [
    "Now to train on our featurized data. We use scikit-learn as in the previous week, because it makes it easy to quickly iterate on different types of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "a_Tp4vWj3OjE",
    "outputId": "dddbeac4-c7a9-4364-c793-7c226d68282c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model = LogisticRegression()\n",
    "baseline_model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WchEBxFeA46-"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "We have a very simple baseline, and it would be interesting if just the features we've created were enough to produce a classification accuracy above 50%, since all we're looking at is the domain nam extension. One natural way to start evaluation of such a simple model is to see how it is doing on the train data. Given that our model only knows a few basic things about the URL, it's not clear whether it will do better than chance on the training data. Sci-kit learn makes computing accuracy on training data easy:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xv_PHkE6346w",
    "outputId": "8d15adf4-215c-4359-cd7a-31ca4af75e4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 0.5906735751295337\n"
     ]
    }
   ],
   "source": [
    "train_y_pred = baseline_model.predict(train_X)\n",
    "print('Train accuracy', accuracy_score(train_y, train_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ylD5OWTZoCz2"
   },
   "source": [
    "We can see that we are not doing very well, but we are doing better than 50%. We can do the same for the val data to see how we are doing on unseen data, which is more valuable for us if we want to make predictions on new websites. Fill in the code below to evaluate val accuracy (~4 minutes)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "syB-Ae66oK-h",
    "outputId": "6907542e-a635-4306-c556-4a567f535764"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy 0.6555555555555556\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "val_y_pred = baseline_model.predict(val_X)\n",
    "### END CODE HERE ###\n",
    "print('Val accuracy', accuracy_score(val_y, val_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hq6sGGVHobjo"
   },
   "source": [
    "We appear to be doing similarly on the val dataset. To better understand the performance of our binary classification model, we should seek to better understand the mistakes that it is making. Specifically, when our model makes a mistake (about 40% of the time), are these mistakes false negatives or false positives?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VeWbCvMa8rhg"
   },
   "source": [
    "To answer these questions, we produce and analyze the confusion matrix. The confusion matrix is a matrix that shows the following:\n",
    "\n",
    "![Confusion Matrix](https://cdn-images-1.medium.com/max/1600/1*Z54JgbS4DUwWSknhDCvNTQ.png)\n",
    "\n",
    "where the terms mean\n",
    "\n",
    "* TP (True Positive) = You predicted positive (fake in our case, since fake has a label of 1) and it’s true.\n",
    "* FP (False Positive) = You predicted positive and it’s false.\n",
    "* FN (False Negative) = You predicted negative and it’s false.\n",
    "* TN (True Negative) = You predicted negative and it’s true.\n",
    "\n",
    "From the confusion matrix, we can extract commonly used metrics like precision (TP/(TP + FP)) and recall (TP/(TP + FN)). Precision quantifies how often the things we classify as positive are actually positive. For our task, this measures what fraction of the sites we classify as fake are actually fake. Recall quantifies what fraction of actually positive examples we classify as positive. In our case, this is the fraction of fake news websites that we actually identify as fake.\n",
    "\n",
    "Finally, a useful score to summarize both precision and recall is the F-1 score. This is just a simple function (the harmonic mean) of precision and recall, shown in the summary below:\n",
    "\n",
    "![Metrics](https://image.noelshack.com/fichiers/2018/20/5/1526651367-qcon-rio-machine-learning-for-everyone-51-638-1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "M9aYm4oqsL6P",
    "outputId": "e2e61018-1980-48ce-8a60-6691e6ffe10d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[50  0]\n",
      " [31  9]]\n"
     ]
    }
   ],
   "source": [
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(val_y, val_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DR2FEk8UsWiq"
   },
   "source": [
    "We can see that we have many false negatives, and not as many false positives. Why is this the case? If we print out *val_y_pred*, we can see that our model is mostly predicting 0's (websites are real)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "gVxJhwhzspF-",
    "outputId": "1c0a33c3-ef6c-4ed9-b389-717948899e0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(val_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eQdhOeigtmCp"
   },
   "source": [
    "Why so many 0's? The only information we are giving our model is its domain name extension. It's natural that the model would learn that websites with \".biz\" extensions are unlikely to be reliable news websites, but it is still the case that most websites in the dataset (fake and real) have \".com\" extensions. Thus, our model will misclassify many fake news websites with \".com\" extensions as real. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "tHQTv6L98uno",
    "outputId": "e3358dc1-3bd5-4153-ff8d-064bb3228d61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0\n",
      "Recall: 0.225\n",
      "F-Score: 0.36734693877551017\n"
     ]
    }
   ],
   "source": [
    "prf = precision_recall_fscore_support(val_y, val_y_pred)\n",
    "\n",
    "print('Precision:', prf[0][1])\n",
    "print('Recall:', prf[1][1])\n",
    "print('F-Score:', prf[2][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vw9xDPTsruaB"
   },
   "source": [
    "Again, the precision and recall metrics suggest that when we classify a website as fake, we are usually right, but we are not doing great at classifying these websites as fake frequently enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L4jjZ5HF35K0"
   },
   "source": [
    "## Using Keywords for a Stronger Baseline\n",
    "\n",
    "The key problem with our model in its current state is that it simply does not have enough information. This should not be a surprise–it was pretty unlikely in the first place that domain name extensions would be enough. If you like, feel free to add a few more extensions in the featurizer above and re-run all the code for evaluation–you'll find it doesn't make much of a difference.\n",
    "\n",
    "Where can we get more information about webpages? From the HTML! Remember that the HTML contains all of the text and structure of a webpage. If we cleverly choose features from the HTML to feed into our logistic regression model, we will drastically improve our performance. We saw yesterday that probing hypotheses related to the counts of hypotheses words produced interesting results, and we will continue in this direction today to produce a model that leverages these differences in word frequencies.\n",
    "\n",
    "The below code introduces a better featurizer that counts the number of keywords (normalized using the *log* function) in the HTML. Normalizing the counts is a trick that prevents the featurized values from becoming too extreme. Read the code and make sure you understand what it is doing. Then add \"sports\" and \"finance\" as additional keywords to expand our model (~3 minutes).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gsadFRbrwWHL"
   },
   "outputs": [],
   "source": [
    "# Gets the log count of a phrase/keyword in HTML (transforming the phrase/keyword\n",
    "# to lowercase).\n",
    "def get_normalized_count(html, phrase):\n",
    "    return math.log(1 + html.count(phrase.lower()))\n",
    "\n",
    "# Returns a dictionary mapping from plaintext feature descriptions to numerical\n",
    "# features for a (url, html) pair.\n",
    "def keyword_featurizer(url, html):\n",
    "    features = {}\n",
    "    \n",
    "    # Same as before.\n",
    "    features['.com domain'] = url.endswith('.com')\n",
    "    features['.org domain'] = url.endswith('.org')\n",
    "    features['.net domain'] = url.endswith('.net')\n",
    "    features['.info domain'] = url.endswith('.info')\n",
    "    features['.org domain'] = url.endswith('.org')\n",
    "    features['.biz domain'] = url.endswith('.biz')\n",
    "    features['.ru domain'] = url.endswith('.ru')\n",
    "    features['.co.uk domain'] = url.endswith('.co.uk')\n",
    "    features['.co domain'] = url.endswith('.co')\n",
    "    features['.tv domain'] = url.endswith('.tv')\n",
    "    features['.news domain'] = url.endswith('.news')\n",
    "    \n",
    "    ### YOUR CODE HERE ###\n",
    "    keywords = ['trump', 'biden', 'clinton', 'sports', 'finance']\n",
    "    ### END CODE HERE\n",
    "    \n",
    "    for keyword in keywords:\n",
    "      features[keyword + ' keyword'] = get_normalized_count(html, keyword)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bgWx3s7UxDue"
   },
   "source": [
    "Let's run and evaluate the above featurizer. Add in code to fit the model, compute train accuracy, val accuracy, val confusion matrix, and val precision, recall, and F1-Score, just as before (~8 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "id": "_DES-HccxG6g",
    "outputId": "e8387222-6e35-415a-807b-b1cdc44d8285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features per example: 15\n",
      "Feature descriptions:\n",
      "('.com domain', '.org domain', '.net domain', '.info domain', '.biz domain', '.ru domain', '.co.uk domain', '.co domain', '.tv domain', '.news domain', 'trump keyword', 'biden keyword', 'clinton keyword', 'sports keyword', 'finance keyword')\n",
      "\n",
      "\n",
      "Train accuracy 0.805699481865285\n",
      "Val accuracy 0.8111111111111111\n",
      "Confusion matrix:\n",
      "[[40 10]\n",
      " [ 7 33]]\n",
      "Precision: 0.7674418604651163\n",
      "Recall: 0.825\n",
      "F-Score: 0.7951807228915662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y, feature_descriptions = prepare_data(train_data, keyword_featurizer)\n",
    "val_X, val_y, feature_descriptions = prepare_data(val_data, keyword_featurizer)\n",
    "\n",
    "print('Number of features per example:', len(train_X[0]))\n",
    "print('Feature descriptions:')\n",
    "print(feature_descriptions)\n",
    "print()\n",
    "  \n",
    "baseline_model = LogisticRegression()\n",
    "\n",
    "### YOUR CODE HERE ###\n",
    "baseline_model.fit(train_X, train_y)\n",
    "print()\n",
    "\n",
    "train_y_pred = baseline_model.predict(train_X)\n",
    "print('Train accuracy', accuracy_score(train_y, train_y_pred))\n",
    "\n",
    "val_y_pred = baseline_model.predict(val_X)\n",
    "print('Val accuracy', accuracy_score(val_y, val_y_pred))\n",
    "\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(val_y, val_y_pred))\n",
    "\n",
    "prf = precision_recall_fscore_support(val_y, val_y_pred)\n",
    "\n",
    "print('Precision:', prf[0][1])\n",
    "print('Recall:', prf[1][1])\n",
    "print('F-Score:', prf[2][1])\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pr29MWpbyHeA"
   },
   "source": [
    "We can see that we are doing dramatically better! The next section addresses how to know which of the above added features made the difference in improving the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1l9Jq6CzwWOt"
   },
   "source": [
    "## Interpreting our Model\n",
    "\n",
    "As mentioned earlier, a key motivation for using a simpler model is interpretability.\n",
    "\n",
    "We've learned that the prediction of a logistic regression classifier is just the output of a multiplication with model weights, followed by a non-linear transformation (sigmoid). Because the sigmoid function is always increasing (monotonic) on its domain (see below), we know that if the dot product (or multiplication of vectors) between model weights and input features is large, then the output prediction will be closer to 1. If the dot product is small, then the output prediction will be closer to 0.\n",
    "\n",
    "![Sigmoid](https://cdn-images-1.medium.com/max/2400/1*RqXFpiNGwdiKBWyLJc_E7g.png)\n",
    "\n",
    "Thus, the weights corresponding to features tell us whether the features are important in the classification. If the weight corresponding to the feature \".net domain\" has a large positive value, then websites with \".net\" domains are more likely to be classified as fake (since fake has label 1). If it has a large negative value, then these websites are more likely to be classified as real. If it has value close to 0, then the feature may not be useful (at least, it may not be useful given that the other features are present).\n",
    "\n",
    "Let's see what weights our model learned. The code below uses *feature_descriptions* and the weights, or coefficients, of the model and sorts them in ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "QHcJkqd4zic8",
    "outputId": "ac169f09-72c5-4851-e355-86f41cd9fecb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sports keyword', -0.9914383185247577),\n",
       " ('.co.uk domain', -0.8873160634555248),\n",
       " ('finance keyword', -0.7659863029847725),\n",
       " ('biden keyword', -0.37088232295973445),\n",
       " ('trump keyword', -0.12615208492554814),\n",
       " ('.com domain', -0.0127152664527546),\n",
       " ('.biz domain', 0.3742897680425671),\n",
       " ('.ru domain', 0.3742897680425671),\n",
       " ('.news domain', 0.3742897680425671),\n",
       " ('.org domain', 0.4371680521273934),\n",
       " ('.info domain', 0.5049555367155103),\n",
       " ('.co domain', 0.9743557117179396),\n",
       " ('.tv domain', 1.0726727152070907),\n",
       " ('clinton keyword', 1.2194725921054308),\n",
       " ('.net domain', 1.2923412213596992)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(feature_descriptions, baseline_model.coef_[0].tolist()), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qfIAQeg5zpRL"
   },
   "source": [
    "What features have positive weight (most predictive of being fake)? Which ones have negative weight (most predictive of being real)? Which ones have close to 0 weight? Are there any feature weights that surprise you? Try coming up with explanations for why the feature weights are the way they are. Does this help you come up with new feature ideas?\n",
    "\n",
    "## Final Baseline\n",
    "\n",
    "Finally, play around with the last few cells, adding more keywords and domain names to see how the results change. Note that \"keywords\" can be a variety of things: English words, English phrases (spaces are allowed), HTML tags, and any other string present in HTML. Also notice how the weights on different features vary–you may observe some interesting effects. When you are done, run the cell below to run evaluations again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "19rbiCbuP8iq",
    "outputId": "b98eac3f-9efa-4e67-c1fb-cf3ccda0f7f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 0.805699481865285\n",
      "Val accuracy 0.8111111111111111\n",
      "Confusion matrix:\n",
      "[[40 10]\n",
      " [ 7 33]]\n",
      "Precision: 0.7674418604651163\n",
      "Recall: 0.825\n",
      "F-Score: 0.7951807228915662\n"
     ]
    }
   ],
   "source": [
    "train_y_pred = baseline_model.predict(train_X)\n",
    "print('Train accuracy', accuracy_score(train_y, train_y_pred))\n",
    "\n",
    "val_y_pred = baseline_model.predict(val_X)\n",
    "print('Val accuracy', accuracy_score(val_y, val_y_pred))\n",
    "\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(val_y, val_y_pred))\n",
    "\n",
    "prf = precision_recall_fscore_support(val_y, val_y_pred)\n",
    "\n",
    "print('Precision:', prf[0][1])\n",
    "print('Recall:', prf[1][1])\n",
    "print('F-Score:', prf[2][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U6aOTjSX0_yj"
   },
   "source": [
    "Congratulations on completing this notebook. Looking at the results of our final baseline, you may be surprised this approach is working at all–after all, our model is still barely looking at the content of websites. We will further explore the issue of modeling the content of websites tomorrow, but as a result of our efforts today, we now know that we can make progress with a relatively simple approach!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jeiPjwrl0iLa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Day 2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
